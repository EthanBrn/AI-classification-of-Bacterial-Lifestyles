{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d734672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import os\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b0a86",
   "metadata": {},
   "source": [
    "# Moving random file to test data dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2f7257b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully moved 300 files to FINALDATA/Test_data/ with their folder names appended.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "034c5e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied 500 files to FINALDATA/Test_data3/ with the renaming scheme.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def select_and_copy_random_files(directories, destination_dir, total_files=300):\n",
    "    # Get the list of all files from each directory\n",
    "    all_files = []\n",
    "    for directory in directories:\n",
    "        files = os.listdir(directory)\n",
    "        all_files.extend(files)\n",
    "\n",
    "    # Ensure there are enough files to select from\n",
    "    if len(all_files) < total_files:\n",
    "        raise ValueError(f\"Not enough files in total. Only {len(all_files)} files available.\")\n",
    "\n",
    "    # Randomly select the required number of files\n",
    "    selected_files = random.sample(all_files, total_files)\n",
    "\n",
    "    # Shuffle the selected files (optional, to add randomness)\n",
    "    random.shuffle(selected_files)\n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    # Copy the selected files to the destination directory\n",
    "    for file in selected_files:\n",
    "        source_file = None\n",
    "        folder_name = None\n",
    "        \n",
    "        # Loop through directories to find the source of the file\n",
    "        for directory in directories:\n",
    "            if file in os.listdir(directory):\n",
    "                source_file = os.path.join(directory, file)\n",
    "                folder_name = os.path.basename(directory)  # Folder name as part of the new filename\n",
    "                break\n",
    "        \n",
    "        # Create a custom renaming scheme. For example: 'folder_name_original_filename'\n",
    "        new_file_name = f\"{folder_name}_{file}\"\n",
    "        destination_file = os.path.join(destination_dir, new_file_name)\n",
    "\n",
    "        # Copy the file (instead of moving it)\n",
    "        shutil.copy(source_file, destination_file)\n",
    "\n",
    "    print(f\"Successfully copied {total_files} files to {destination_dir} with the renaming scheme.\")\n",
    "\n",
    "# List of directories to select files from\n",
    "directories = [\n",
    "    \"FINALDATA/Split_Data/Evolved+disk/EDNS\",\n",
    "    \"FINALDATA/Split_Data/Evolved+disk/EDS\",\n",
    "    \"FINALDATA/Split_Data/Evolved/ENS\",\n",
    "    \"FINALDATA/Split_Data/Evolved/ES\",  # Add your 4th directory\n",
    "    \"FINALDATA/Split_Data/WT/WTNS\",  # Add your 5th directory\n",
    "    \"FINALDATA/Split_Data/WT/WTS\",  # Add your 6th directory\n",
    "]\n",
    "\n",
    "destination_dir = \"FINALDATA/Test_data3/\"\n",
    "\n",
    "select_and_copy_random_files(directories, destination_dir, total_files=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bbcd5e",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e50ad220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_displacement_optimised(file_path):\n",
    "    data = pd.read_csv(file_path, delim_whitespace=True, header=None, \n",
    "                       names=['time', 'x', 'y', 'z', 'x_smooth', 'y_smooth', 'z_smooth'])\n",
    "    \n",
    "    x_smooth = np.array(data['x_smooth'])\n",
    "    y_smooth = np.array(data['y_smooth'])\n",
    "    z_smooth = np.array(data['z_smooth'])\n",
    "    time = np.array(data['time'])\n",
    "    \n",
    "    msd_by_tau = {}\n",
    "\n",
    "    for tau in range(1, len(time)):\n",
    "        \n",
    "        dx = x_smooth[tau:] - x_smooth[:-tau]\n",
    "        dy = y_smooth[tau:] - y_smooth[:-tau]\n",
    "        dz = z_smooth[tau:] - z_smooth[:-tau]\n",
    "        squared_displacements = dx**2 + dy**2 + dz**2\n",
    "        msd_by_tau[round(time[tau] - time[0], 3)] = np.mean(squared_displacements)\n",
    "\n",
    "    return msd_by_tau\n",
    "\n",
    "\n",
    "def find_gradient_intercept(file_path):\n",
    "    msd_by_tau = mean_squared_displacement_optimised(file_path)\n",
    "    \n",
    "    # Check if msd_by_tau has enough data to perform regression\n",
    "    if len(msd_by_tau) < 2:\n",
    "        return np.nan, np.nan  # Not enough data to perform linear regression\n",
    "    \n",
    "    tau = np.array(list(msd_by_tau.keys()))\n",
    "    msd_arr = np.array(list(msd_by_tau.values()))\n",
    "    \n",
    "    log_tau = np.log(tau)\n",
    "    log_msd = np.log(msd_arr)\n",
    "    \n",
    "    # Perform linear regression if the data is valid\n",
    "    try:\n",
    "        slope, intercept, _, _, _ = linregress(log_tau, log_msd)\n",
    "        return slope, intercept\n",
    "    except ValueError:\n",
    "        return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82e8ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_curvature_arr(file_path):\n",
    "    data = pd.read_csv(file_path, delim_whitespace=True, header=None, \n",
    "                       names=['time', 'x', 'y', 'z', 'x_smooth', 'y_smooth', 'z_smooth'])\n",
    "    if len(data) <= 2:\n",
    "        return np.array([])\n",
    "    v_arr = np.zeros(len(data)) #init velocity array \n",
    "    t_arr = np.zeros(len(data))  #init time array    \n",
    "    T_arr = np.zeros((len(data)-2, 4))  # 2D array with time + 3 components t, x,y,z\n",
    "\n",
    "    for i in range(1, len(data)-1):\n",
    "\n",
    "    \n",
    "        x_smooth = np.array(data['x_smooth'])  #Extracting data\n",
    "        y_smooth = np.array(data['y_smooth'])\n",
    "        z_smooth = np.array(data['z_smooth'])\n",
    "        time = np.array(data['time'])\n",
    "    \n",
    "        t_next = time[i+1]       #Define points for central difference\n",
    "        x_next = x_smooth[i+1]\n",
    "        y_next = y_smooth[i+1]\n",
    "        z_next = z_smooth[i+1]\n",
    "    \n",
    "        t_prev = time[i-1]\n",
    "        x_prev = x_smooth[i-1]\n",
    "        y_prev = y_smooth[i-1]\n",
    "        z_prev = z_smooth[i-1]\n",
    "        dir_vector = np.array([x_next - x_prev, y_next - y_prev, z_next - z_prev])\n",
    "        T_vector = dir_vector / np.linalg.norm(dir_vector)\n",
    "        T_arr[i-1] = np.array([time[i], *T_vector])  \n",
    "    \n",
    "        r_diff = np.sqrt((x_next-x_prev)**2+(y_next-y_prev)**2+(z_next-z_prev)**2)\n",
    "        dt = t_next - t_prev\n",
    "        v = r_diff/ (dt)\n",
    "\n",
    "        v_arr[i] = v\n",
    "    time_for_T = time[1:-1] # Time associated with the T vectors, lost first and last point \n",
    "        \n",
    "    v_arr = v_arr[2:-2]    #velocity for final calculation. Los first and last two points\n",
    "        \n",
    "\n",
    "    dT_arr = np.zeros((len(T_arr)-2, 4))  #init dT array, loses frist and last point from T_arr\n",
    "    \n",
    "    for i in range(1, len(T_arr)-1):\n",
    "        dT = (T_arr[i+1, 1:] - T_arr[i-1, 1:]) / (T_arr[i+1, 0] - T_arr[i-1, 0])   #central diff for dT/dt \n",
    "        dT_arr[i-1] = np.array([T_arr[i, 0], *dT])#Put associated t x, y ,z in array\n",
    "        \n",
    "    curvature_arr = np.zeros(len(dT_arr))    #init curve array \n",
    "    \n",
    "    for i in range(len(v_arr)):\n",
    "        \n",
    "        dT_segment = dT_arr[i, 1:]    #extract only x y z \n",
    "        dT_magnitude = np.linalg.norm(dT_segment)  #mag of x y z vector \n",
    "        v_mag = v_arr[i] #veloctity associated with this point \n",
    "        \n",
    "        curvature = dT_magnitude / v_mag  #calculate curvature\n",
    "        curvature_arr[i] = curvature   #put in array \n",
    "    times_for_curvature = time[2:-2]   #Time associated with the curvature array.\n",
    "    \n",
    "    return curvature_arr , times_for_curvature  #Return curvatre and associated time array for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bdce4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "def calc_acc(file_path):\n",
    "    \n",
    "    data = pd.read_csv(file_path, delim_whitespace=True, header=None, names=['time', 'x', 'y', 'z', 'x_smooth', 'y_smooth', 'z_smooth'])\n",
    "    if len(data) <= 2:\n",
    "        return np.array([])\n",
    "    v_arr = np.zeros(len(data))\n",
    "    t_arr = np.zeros(len(data)-1)\n",
    "    a_arr = np.zeros(len(data)-2)\n",
    "    for i in range(1,len(data)-1):\n",
    "        data = pd.read_csv(file_path, delim_whitespace=True, header=None, \n",
    "                       names=['time', 'x', 'y', 'z', 'x_smooth', 'y_smooth', 'z_smooth'])\n",
    "    \n",
    "        x_smooth = np.array(data['x_smooth'])\n",
    "        y_smooth = np.array(data['y_smooth'])\n",
    "        z_smooth = np.array(data['z_smooth'])\n",
    "        time = np.array(data['time'])\n",
    "    \n",
    "        t_next = time[i+1]\n",
    "        x_next = x_smooth[i+1]\n",
    "        y_next = y_smooth[i+1]\n",
    "        z_next = z_smooth[i+1]\n",
    "    \n",
    "        t_prev = time[i-1]\n",
    "        x_prev = x_smooth[i-1]\n",
    "        y_prev = y_smooth[i-1]\n",
    "        z_prev = z_smooth[i-1]\n",
    "        r_diff = np.sqrt((x_next-x_prev)**2+(y_next-y_prev)**2+(z_next-z_prev)**2)\n",
    "        dt = t_next - t_prev\n",
    "  \n",
    "  \n",
    "    #velocity at the current point using central difference\n",
    "        \n",
    "    \n",
    "        v_arr[i] = r_diff / dt\n",
    "    \n",
    "    t_arr = time[1:-1]\n",
    "    v_arr = v_arr[1:-1]\n",
    "\n",
    "        \n",
    "    for i in range(1, len(v_arr) - 1):\n",
    "        dv = (v_arr[i + 1] - v_arr[i - 1])\n",
    "        dt = time[i + 1] - time[i - 1]\n",
    "        a_arr[i] = dv / dt\n",
    "    t_for_acc = time [2:-2] \n",
    "    a_arr = a_arr[1:-1]   \n",
    "    return a_arr , t_for_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5236c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_velo_arr(file_path):\n",
    "    data = pd.read_csv(file_path, delim_whitespace=True, header=None, \n",
    "                       names=['time', 'x', 'y', 'z', 'x_smooth', 'y_smooth', 'z_smooth'])\n",
    "    x_smooth = np.array(data['x_smooth'])\n",
    "    y_smooth = np.array(data['y_smooth'])\n",
    "    z_smooth = np.array(data['z_smooth'])\n",
    "    time = np.array(data['time'])\n",
    "    if len(data) <= 2:\n",
    "        return np.array([])\n",
    "    v_arr = np.zeros(len(data)-2)\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "    for i in range(1,len(data)-1):\n",
    "        t_next = time[i+1]\n",
    "        x_next = x_smooth[i+1]\n",
    "        y_next = y_smooth[i+1]\n",
    "        z_next = z_smooth[i+1]\n",
    "    \n",
    "        t_prev = time[i-1]\n",
    "        x_prev = x_smooth[i-1]\n",
    "        y_prev = y_smooth[i-1]\n",
    "        z_prev = z_smooth[i-1]\n",
    "    \n",
    "    # Calculate radial distance for next and previous points\n",
    " \n",
    "    \n",
    "    \n",
    "        r_diff = np.sqrt((x_next-x_prev)**2+(y_next-y_prev)**2+(z_next-z_prev)**2)\n",
    "        dt = t_next - t_prev\n",
    "  \n",
    "  \n",
    "    #velocity at the current point using central difference\n",
    "        v = r_diff/ (dt)\n",
    "\n",
    "        v_arr[i-1] = v\n",
    "        \n",
    "    return v_arr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5508e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_path_length(file_path):\n",
    "    # Read the data from the .txt file\n",
    "    data = pd.read_csv(file_path, delim_whitespace=True, header=None,\n",
    "                       names=['time', 'x', 'y', 'z', 'x_smooth', 'y_smooth', 'z_smooth'])\n",
    "    \n",
    "    # Extract x_smooth, y_smooth, and z_smooth for path length calculation\n",
    "    x_smooth = np.array(data['x_smooth'])\n",
    "    y_smooth = np.array(data['y_smooth'])\n",
    "    z_smooth = np.array(data['z_smooth'])\n",
    "\n",
    "    # Calculate the path length as the sum of the distances between consecutive points\n",
    "    path_length = np.sum(np.sqrt(np.diff(x_smooth)**2 + np.diff(y_smooth)**2 + np.diff(z_smooth)**2))\n",
    "    \n",
    "    return path_length\n",
    "\n",
    "def calculate_straight_distance(file_path):\n",
    "    # Read the data from the .txt file\n",
    "    data = pd.read_csv(file_path, delim_whitespace=True, header=None,\n",
    "                       names=['time', 'x', 'y', 'z', 'x_smooth', 'y_smooth', 'z_smooth'])\n",
    "    \n",
    "    # Extract starting and ending coordinates\n",
    "    start_point = data.iloc[0][['x_smooth', 'y_smooth', 'z_smooth']].to_numpy()\n",
    "    end_point = data.iloc[-1][['x_smooth', 'y_smooth', 'z_smooth']].to_numpy()\n",
    "    \n",
    "    # Calculate the straight-line distance\n",
    "    straight_distance = np.linalg.norm(end_point - start_point)\n",
    "    \n",
    "    return straight_distance\n",
    "\n",
    "def calculate_tortuosity(file_path):\n",
    "    path_length = calculate_path_length(file_path)\n",
    "    straight_distance = calculate_straight_distance(file_path)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if straight_distance == 0:\n",
    "        return np.inf  # Tortuosity is infinite if the straight distance is zero\n",
    "    \n",
    "    # Tortuosity calculation\n",
    "    tortuosity = path_length / straight_distance\n",
    "    return tortuosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6e82d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorientation_events_per_second(file_path):\n",
    "    data = pd.read_csv(file_path, delim_whitespace=True, header=None,\n",
    "                       names=['time', 'x', 'y', 'z', 'x_smooth', 'y_smooth', 'z_smooth'])\n",
    "    \n",
    "    x_smooth = np.array(data['x_smooth'])\n",
    "    y_smooth = np.array(data['y_smooth'])\n",
    "    z_smooth = np.array(data['z_smooth'])\n",
    "    time = data['time']\n",
    "    \n",
    "    positions = np.stack((x_smooth, y_smooth, z_smooth), axis=1)\n",
    "\n",
    "    # Calculate differences in positional vectors between consecutive points\n",
    "    dir_vectors = np.diff(positions, axis=0)\n",
    "    # Calculate magnitudes of vectors\n",
    "    mags = np.linalg.norm(dir_vectors, axis=1)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    non_zero_mags = mags > 0  \n",
    "    unit_dir = np.zeros_like(dir_vectors)  \n",
    "    unit_dir[non_zero_mags] = dir_vectors[non_zero_mags] / mags[non_zero_mags, np.newaxis]\n",
    "\n",
    "    # Calculate dot products k steps ahead\n",
    "    k = 5\n",
    "    dot_products = [np.dot(unit_dir[i], unit_dir[i + k]) for i in range(len(unit_dir) - k) if i + k < len(unit_dir)]\n",
    "\n",
    "    change_in_dir_threshold = 0.98\n",
    "    consecutive_low_values = 3\n",
    "\n",
    "    low_sequences = []\n",
    "    current_sequence = []\n",
    "\n",
    "    for dp in dot_products:\n",
    "        if dp < change_in_dir_threshold:\n",
    "            current_sequence.append(dp)\n",
    "        else:\n",
    "            if len(current_sequence) >= consecutive_low_values:\n",
    "                low_sequences.append(current_sequence)\n",
    "            current_sequence = []  # Reset for the next sequence\n",
    "\n",
    "    if len(current_sequence) >= consecutive_low_values:\n",
    "        low_sequences.append(current_sequence)\n",
    "\n",
    "    # Calculate events per second\n",
    "    events_per_sec = len(low_sequences) / (np.max(time) - np.min(time))\n",
    "    return events_per_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e4f1b",
   "metadata": {},
   "source": [
    "# Creating DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40de08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_features_to_array(file_path, min_length=10):\n",
    "    # Extract features from file\n",
    "    msd_by_tau = mean_squared_displacement_optimised(file_path)\n",
    "    velocity = calc_velo_arr(file_path)\n",
    "    acceleration = calc_acc(file_path)\n",
    "    reorientations_per_sec = reorientation_events_per_second(file_path)\n",
    "    curvature = calc_curvature_arr(file_path)\n",
    "    tortuosity = calculate_tortuosity(file_path)\n",
    "    path_length = calculate_path_length(file_path)\n",
    "    slope, intercept = find_gradient_intercept(file_path)\n",
    "    \n",
    "    # Check the length of the data (if track is too short, skip it)\n",
    "    if len(msd_by_tau) < min_length or len(velocity) < min_length:\n",
    "        return None  # Skip tracks that are too short\n",
    "    \n",
    "    # Ensure features arrays have values, or use np.nan if empty\n",
    "    msd_arr = np.array(list(msd_by_tau.values())) if len(msd_by_tau) > 0 else np.nan\n",
    "    log_velocity = np.log(velocity[velocity > 0]) if len(velocity) > 0 else np.nan\n",
    "    \n",
    "    # Return exactly 21 features (track_type, filename, and 19 features)\n",
    "    features = [\n",
    "        \n",
    "        slope if slope else np.nan,\n",
    "        intercept if intercept else np.nan,\n",
    "        # Placeholder for gradient and intercept (if not calculated)\n",
    "        np.mean(log_velocity) if len(log_velocity) > 0 else np.nan,\n",
    "        np.std(log_velocity) if len(log_velocity) > 0 else np.nan,\n",
    "        np.max(log_velocity) if len(log_velocity) > 0 else np.nan,\n",
    "        np.min(log_velocity) if len(log_velocity) > 0 else np.nan,  # Log velocity features\n",
    "        np.mean(acceleration) if len(acceleration) > 0 else np.nan, \n",
    "        np.std(acceleration) if len(acceleration) > 0 else np.nan,\n",
    "        np.max(acceleration) if len(acceleration) > 0 else np.nan, \n",
    "        np.min(acceleration) if len(acceleration) > 0 else np.nan,  # Acceleration features\n",
    "        reorientations_per_sec if reorientations_per_sec else np.nan,  # Reorientation events\n",
    "        np.mean(curvature) if len(curvature) > 0 else np.nan,\n",
    "        np.std(curvature) if len(curvature) > 0 else np.nan,\n",
    "        np.max(curvature) if len(curvature) > 0 else np.nan,  # Curvature features\n",
    "        tortuosity if tortuosity else np.nan, \n",
    "        path_length if path_length else np.nan  # Tortuosity and path length\n",
    "    ]\n",
    "    \n",
    "    return features\n",
    "\n",
    "def feature_df(directory_path, min_length=10):\n",
    "    all_features = []  # List to store valid feature data\n",
    "    \n",
    "    # Iterate through files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        if os.path.isfile(file_path):  # Ensure it's a file\n",
    "            features = extract_features_to_array(file_path, min_length)\n",
    "            \n",
    "            if features is not None:  # Only add valid tracks\n",
    "                track_type = filename.split('_')[0]  # Assuming type is in the filename\n",
    "                all_features.append([track_type, filename] + features)\n",
    "    \n",
    "    # Remove any None values (files that didn't meet length threshold)\n",
    "    all_features = [feature for feature in all_features if feature is not None]\n",
    "    \n",
    "    # Convert list of valid features to a DataFrame\n",
    "    df = pd.DataFrame(all_features, columns=[\n",
    "        'track_type', 'filename',\n",
    "        'gradient', 'intercept', 'mean_log_velocity', 'stddev_log_velocity',\n",
    "        'max_log_velocity', 'min_log_velocity', 'mean_acceleration', 'stddev_acceleration',\n",
    "        'max_acceleration', 'min_acceleration', 'reorientations_per_sec', 'mean_curvature',\n",
    "        'stddev_curvature', 'max_curvature', 'tortuosity', 'path_length'\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b828a90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-adf42148a9bd>:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "  T_vector = dir_vector / np.linalg.norm(dir_vector)\n",
      "<ipython-input-7-adf42148a9bd>:55: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  curvature = dT_magnitude / v_mag  #calculate curvature\n",
      "<ipython-input-7-adf42148a9bd>:55: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  curvature = dT_magnitude / v_mag  #calculate curvature\n"
     ]
    }
   ],
   "source": [
    "df = feature_df('FINALDATA/Test_data3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c31be50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_type</th>\n",
       "      <th>filename</th>\n",
       "      <th>gradient</th>\n",
       "      <th>intercept</th>\n",
       "      <th>mean_log_velocity</th>\n",
       "      <th>stddev_log_velocity</th>\n",
       "      <th>max_log_velocity</th>\n",
       "      <th>min_log_velocity</th>\n",
       "      <th>mean_acceleration</th>\n",
       "      <th>stddev_acceleration</th>\n",
       "      <th>max_acceleration</th>\n",
       "      <th>min_acceleration</th>\n",
       "      <th>reorientations_per_sec</th>\n",
       "      <th>mean_curvature</th>\n",
       "      <th>stddev_curvature</th>\n",
       "      <th>max_curvature</th>\n",
       "      <th>tortuosity</th>\n",
       "      <th>path_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENS</td>\n",
       "      <td>ENS_track81208_0_processed.txt</td>\n",
       "      <td>1.701232</td>\n",
       "      <td>4.237139</td>\n",
       "      <td>2.133661</td>\n",
       "      <td>0.898218</td>\n",
       "      <td>3.536060</td>\n",
       "      <td>-0.360211</td>\n",
       "      <td>9.557535</td>\n",
       "      <td>248.384852</td>\n",
       "      <td>968.808509</td>\n",
       "      <td>-606.917541</td>\n",
       "      <td>6.976744</td>\n",
       "      <td>9.561206</td>\n",
       "      <td>24.157304</td>\n",
       "      <td>263.837759</td>\n",
       "      <td>1.776933</td>\n",
       "      <td>10.287340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EDNS</td>\n",
       "      <td>EDNS_track54912_0_processed.txt</td>\n",
       "      <td>1.606683</td>\n",
       "      <td>4.109765</td>\n",
       "      <td>2.447608</td>\n",
       "      <td>0.644055</td>\n",
       "      <td>3.514067</td>\n",
       "      <td>0.068679</td>\n",
       "      <td>11.401612</td>\n",
       "      <td>242.850418</td>\n",
       "      <td>762.773742</td>\n",
       "      <td>-848.551621</td>\n",
       "      <td>9.917355</td>\n",
       "      <td>6.813065</td>\n",
       "      <td>11.564500</td>\n",
       "      <td>141.513504</td>\n",
       "      <td>1.907978</td>\n",
       "      <td>8.311156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ES</td>\n",
       "      <td>ES_track132939_0_processed.txt</td>\n",
       "      <td>1.801404</td>\n",
       "      <td>6.427257</td>\n",
       "      <td>3.098041</td>\n",
       "      <td>0.744236</td>\n",
       "      <td>4.277293</td>\n",
       "      <td>1.206329</td>\n",
       "      <td>6.949521</td>\n",
       "      <td>246.013576</td>\n",
       "      <td>972.965287</td>\n",
       "      <td>-1097.691499</td>\n",
       "      <td>6.930693</td>\n",
       "      <td>6.025760</td>\n",
       "      <td>5.480151</td>\n",
       "      <td>31.300272</td>\n",
       "      <td>1.554178</td>\n",
       "      <td>28.870975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EDNS</td>\n",
       "      <td>EDNS_track34474_0_processed.txt</td>\n",
       "      <td>0.890607</td>\n",
       "      <td>0.350629</td>\n",
       "      <td>1.490617</td>\n",
       "      <td>1.195546</td>\n",
       "      <td>2.680981</td>\n",
       "      <td>-2.453408</td>\n",
       "      <td>11.947559</td>\n",
       "      <td>107.573897</td>\n",
       "      <td>363.710511</td>\n",
       "      <td>-410.032948</td>\n",
       "      <td>7.734807</td>\n",
       "      <td>20.707341</td>\n",
       "      <td>134.421856</td>\n",
       "      <td>2253.366614</td>\n",
       "      <td>5.884277</td>\n",
       "      <td>6.002668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EDNS</td>\n",
       "      <td>EDNS_track92165_0_processed.txt</td>\n",
       "      <td>0.754142</td>\n",
       "      <td>1.276057</td>\n",
       "      <td>1.964142</td>\n",
       "      <td>1.042575</td>\n",
       "      <td>3.739174</td>\n",
       "      <td>-0.719390</td>\n",
       "      <td>33.195348</td>\n",
       "      <td>225.044808</td>\n",
       "      <td>1122.456099</td>\n",
       "      <td>-707.501607</td>\n",
       "      <td>8.474576</td>\n",
       "      <td>15.119860</td>\n",
       "      <td>36.129708</td>\n",
       "      <td>402.859431</td>\n",
       "      <td>2.897660</td>\n",
       "      <td>7.020181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>ENS</td>\n",
       "      <td>ENS_track252767_0_processed.txt</td>\n",
       "      <td>1.675585</td>\n",
       "      <td>2.109512</td>\n",
       "      <td>1.427313</td>\n",
       "      <td>0.538436</td>\n",
       "      <td>2.083858</td>\n",
       "      <td>0.119277</td>\n",
       "      <td>10.156549</td>\n",
       "      <td>49.253875</td>\n",
       "      <td>176.379752</td>\n",
       "      <td>-159.076280</td>\n",
       "      <td>5.106383</td>\n",
       "      <td>13.662256</td>\n",
       "      <td>10.839919</td>\n",
       "      <td>81.865328</td>\n",
       "      <td>1.706384</td>\n",
       "      <td>5.552453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>WTS</td>\n",
       "      <td>WTS_track21_0 (2)_processed.txt</td>\n",
       "      <td>1.879332</td>\n",
       "      <td>5.638844</td>\n",
       "      <td>2.885362</td>\n",
       "      <td>0.229033</td>\n",
       "      <td>3.586314</td>\n",
       "      <td>2.011337</td>\n",
       "      <td>2.677165</td>\n",
       "      <td>69.158460</td>\n",
       "      <td>378.385373</td>\n",
       "      <td>-700.869047</td>\n",
       "      <td>6.238532</td>\n",
       "      <td>2.261897</td>\n",
       "      <td>2.493457</td>\n",
       "      <td>8.175000</td>\n",
       "      <td>1.306811</td>\n",
       "      <td>150.303455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>EDNS</td>\n",
       "      <td>EDNS_track129486_0_processed.txt</td>\n",
       "      <td>1.468388</td>\n",
       "      <td>5.684454</td>\n",
       "      <td>3.246148</td>\n",
       "      <td>0.702201</td>\n",
       "      <td>3.969870</td>\n",
       "      <td>0.790571</td>\n",
       "      <td>31.706242</td>\n",
       "      <td>225.128408</td>\n",
       "      <td>1357.149690</td>\n",
       "      <td>-790.622786</td>\n",
       "      <td>6.593407</td>\n",
       "      <td>14.201588</td>\n",
       "      <td>13.474938</td>\n",
       "      <td>59.022597</td>\n",
       "      <td>1.858490</td>\n",
       "      <td>28.252718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>ES</td>\n",
       "      <td>ES_track22172_0_processed.txt</td>\n",
       "      <td>1.778058</td>\n",
       "      <td>3.433815</td>\n",
       "      <td>1.879095</td>\n",
       "      <td>0.586836</td>\n",
       "      <td>2.630216</td>\n",
       "      <td>-0.185258</td>\n",
       "      <td>2.249052</td>\n",
       "      <td>95.451606</td>\n",
       "      <td>321.865860</td>\n",
       "      <td>-400.614630</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>4.922717</td>\n",
       "      <td>14.870578</td>\n",
       "      <td>200.181148</td>\n",
       "      <td>1.448251</td>\n",
       "      <td>6.099553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>ENS</td>\n",
       "      <td>ENS_track143447_5_processed.txt</td>\n",
       "      <td>1.572139</td>\n",
       "      <td>3.121780</td>\n",
       "      <td>1.789403</td>\n",
       "      <td>0.678156</td>\n",
       "      <td>3.386779</td>\n",
       "      <td>-1.313237</td>\n",
       "      <td>11.914375</td>\n",
       "      <td>95.745021</td>\n",
       "      <td>662.324803</td>\n",
       "      <td>-459.124585</td>\n",
       "      <td>6.305833</td>\n",
       "      <td>15.772612</td>\n",
       "      <td>19.584690</td>\n",
       "      <td>706.477855</td>\n",
       "      <td>2.369113</td>\n",
       "      <td>71.278509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>814 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    track_type                          filename  gradient  intercept  \\\n",
       "0          ENS    ENS_track81208_0_processed.txt  1.701232   4.237139   \n",
       "1         EDNS   EDNS_track54912_0_processed.txt  1.606683   4.109765   \n",
       "2           ES    ES_track132939_0_processed.txt  1.801404   6.427257   \n",
       "3         EDNS   EDNS_track34474_0_processed.txt  0.890607   0.350629   \n",
       "4         EDNS   EDNS_track92165_0_processed.txt  0.754142   1.276057   \n",
       "..         ...                               ...       ...        ...   \n",
       "809        ENS   ENS_track252767_0_processed.txt  1.675585   2.109512   \n",
       "810        WTS   WTS_track21_0 (2)_processed.txt  1.879332   5.638844   \n",
       "811       EDNS  EDNS_track129486_0_processed.txt  1.468388   5.684454   \n",
       "812         ES     ES_track22172_0_processed.txt  1.778058   3.433815   \n",
       "813        ENS   ENS_track143447_5_processed.txt  1.572139   3.121780   \n",
       "\n",
       "     mean_log_velocity  stddev_log_velocity  max_log_velocity  \\\n",
       "0             2.133661             0.898218          3.536060   \n",
       "1             2.447608             0.644055          3.514067   \n",
       "2             3.098041             0.744236          4.277293   \n",
       "3             1.490617             1.195546          2.680981   \n",
       "4             1.964142             1.042575          3.739174   \n",
       "..                 ...                  ...               ...   \n",
       "809           1.427313             0.538436          2.083858   \n",
       "810           2.885362             0.229033          3.586314   \n",
       "811           3.246148             0.702201          3.969870   \n",
       "812           1.879095             0.586836          2.630216   \n",
       "813           1.789403             0.678156          3.386779   \n",
       "\n",
       "     min_log_velocity  mean_acceleration  stddev_acceleration  \\\n",
       "0           -0.360211           9.557535           248.384852   \n",
       "1            0.068679          11.401612           242.850418   \n",
       "2            1.206329           6.949521           246.013576   \n",
       "3           -2.453408          11.947559           107.573897   \n",
       "4           -0.719390          33.195348           225.044808   \n",
       "..                ...                ...                  ...   \n",
       "809          0.119277          10.156549            49.253875   \n",
       "810          2.011337           2.677165            69.158460   \n",
       "811          0.790571          31.706242           225.128408   \n",
       "812         -0.185258           2.249052            95.451606   \n",
       "813         -1.313237          11.914375            95.745021   \n",
       "\n",
       "     max_acceleration  min_acceleration  reorientations_per_sec  \\\n",
       "0          968.808509       -606.917541                6.976744   \n",
       "1          762.773742       -848.551621                9.917355   \n",
       "2          972.965287      -1097.691499                6.930693   \n",
       "3          363.710511       -410.032948                7.734807   \n",
       "4         1122.456099       -707.501607                8.474576   \n",
       "..                ...               ...                     ...   \n",
       "809        176.379752       -159.076280                5.106383   \n",
       "810        378.385373       -700.869047                6.238532   \n",
       "811       1357.149690       -790.622786                6.593407   \n",
       "812        321.865860       -400.614630                6.250000   \n",
       "813        662.324803       -459.124585                6.305833   \n",
       "\n",
       "     mean_curvature  stddev_curvature  max_curvature  tortuosity  path_length  \n",
       "0          9.561206         24.157304     263.837759    1.776933    10.287340  \n",
       "1          6.813065         11.564500     141.513504    1.907978     8.311156  \n",
       "2          6.025760          5.480151      31.300272    1.554178    28.870975  \n",
       "3         20.707341        134.421856    2253.366614    5.884277     6.002668  \n",
       "4         15.119860         36.129708     402.859431    2.897660     7.020181  \n",
       "..              ...               ...            ...         ...          ...  \n",
       "809       13.662256         10.839919      81.865328    1.706384     5.552453  \n",
       "810        2.261897          2.493457       8.175000    1.306811   150.303455  \n",
       "811       14.201588         13.474938      59.022597    1.858490    28.252718  \n",
       "812        4.922717         14.870578     200.181148    1.448251     6.099553  \n",
       "813       15.772612         19.584690     706.477855    2.369113    71.278509  \n",
       "\n",
       "[814 rows x 18 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbed4a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining NaNs: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df.select_dtypes(include=['float64', 'int64'])\n",
    "#X = X.drop(['min_log_velocity','max_log_velocity','max_acceleration', 'min_acceleration', 'max_curvature'], axis=1)\n",
    "\n",
    "\n",
    "# Replace NaN values with the mean of each column\n",
    "X_filled = X.apply(lambda col: col.fillna(col.mean()), axis=0)\n",
    "\n",
    "# Confirm there are no NaN values left\n",
    "print(\"Remaining NaNs:\", X_filled.isna().sum().sum())  # Should output 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7413154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_filled)\n",
    "y = df['track_type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filled, y, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6e7920e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.20%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # StandardScaler for preprocessing\n",
    "    ('mlp', MLPClassifier(hidden_layer_sizes=(50, 30), activation='tanh', \n",
    "                          solver='adam', alpha=0.001, learning_rate_init=0.1, \n",
    "                          max_iter=20000, early_stopping=False))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e9639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9babff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.29%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define a pipeline without grid search\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Standardize features\n",
    "    ('pca', PCA(n_components=11)),  # Use 15 principal components to reduce dimensionality\n",
    "    ('classifier', VotingClassifier(  # Ensemble of three classifiers\n",
    "        estimators=[\n",
    "            ('mlp', MLPClassifier(max_iter=1000, hidden_layer_sizes=(50, 30), alpha=0.001, learning_rate_init=0.01)),\n",
    "            ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "            ('gb', GradientBoostingClassifier(learning_rate=0.1, n_estimators=100))\n",
    "        ],\n",
    "        voting='soft'  # Soft voting to average probabilities from all classifiers\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print the accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0a2cc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__gb__n_estimators': 100, 'classifier__mlp__hidden_layer_sizes': (50, 30), 'classifier__rf__n_estimators': 150, 'pca__n_components': 11}\n",
      "Best Cross-Validation Score: 59.29%\n",
      "Test Accuracy with Best Model: 70.73%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Simplified parameter grid\n",
    "param_grid = {\n",
    "    'pca__n_components': [10, 11],  # Fewer PCA components to test\n",
    "    'classifier__mlp__hidden_layer_sizes': [(50, 30), (60, 40)],  # Fewer hidden layer sizes for MLP\n",
    "    'classifier__rf__n_estimators': [100, 150],  # Only two options for RandomForest trees\n",
    "    'classifier__gb__n_estimators': [100, 150]  # Fewer boosting stages for GradientBoosting\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with fewer options and a 3-fold CV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # 3-fold cross-validation to reduce time\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    scoring='accuracy'  # Use accuracy as the metric\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Print the accuracy score of the best model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy with Best Model: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec48b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "e25d516f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  1,  7,  0,  0,  0],\n",
       "       [ 0, 12,  0,  3,  0,  1],\n",
       "       [ 7,  0, 19,  0,  0,  0],\n",
       "       [ 0,  4,  0,  2,  0,  2],\n",
       "       [ 0,  0,  0,  0,  1,  1],\n",
       "       [ 0,  3,  0,  0,  0, 10]])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "89ab93f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc79b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53badaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
